{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f143df9d",
   "metadata": {},
   "source": [
    "# Packed Data Formats\n",
    "\n",
    "In this tutorial we will review two ways of saving our data that is then loaded in PyTorch\n",
    "\n",
    "\n",
    "## Scenario\n",
    "Imagine we want to train a network which can classify images accurately. Here we will the ImageNet dataset to train our network. ImageNet is a huge image database of 14 million images. Below are a few example images from this dataset:\n",
    "\n",
    "<img src=\"src/imagenet.jpg\" alt= \"ImageNet\" width=\"500\" height=\"500\">\n",
    "\n",
    "So you download the dataset to your computer and unzip to see the images inside. However, extracting 14 million images is something your computer does not like for a couple of reasons:\n",
    "- extracting individual files takes long\n",
    "- all files are stored individually with their own metadata\n",
    "- on Snellius, we mostly have a shared disk so whatever data-intense heavy tasks are running may impact other users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed5ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "#DATA_PATH = os.getenv(\"TEACHER_DIR\", os.getcwd()) + \"/JHS_data\"\n",
    "DATA_PATH = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7062c5e8",
   "metadata": {},
   "source": [
    "## Create a torch Dataset class for jpeg images\n",
    "\n",
    "After we extracted our imagenet zip files, we are left with .jpeg images. To load these images during training, we write our own Dataset class. \n",
    "\n",
    "NB: for this tutorial, we will work with a subset of ImageNet of 10k images and corresponding labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f204a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JpegDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"ImageNet10k Dataset for Jpeg images to pass to a PyTorch dataloader\n",
    "\n",
    "    Args:\n",
    "        path (str): location where the images are stored on disk\n",
    "        transform (obj): torchvision.transforms object or None\n",
    "    Returns:\n",
    "        torch Dataset: to pass to a dataloader\n",
    "    \"\"\"\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "        self._get_img_names()\n",
    "    \n",
    "        label_fname = \"validation_labels_10k.txt\"\n",
    "        self._read_label_file(label_fname)\n",
    "\n",
    "    def _get_img_names(self):\n",
    "        self.img_names = sorted(glob.glob(f\"{self.path}/*.JPEG\"))\n",
    "        \n",
    "    def _read_label_file(self, label_fname):\n",
    "        with open(os.path.join(self.path, label_fname), \"r\") as file:\n",
    "            self.labels = [int(l) for l in file.readlines()]\n",
    "\n",
    "    @staticmethod\n",
    "    def read_image(img_name):\n",
    "        image = PIL.Image.open(img_name).convert(\"RGB\")\n",
    "        return image\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name= self.img_names[index]\n",
    "        image = self.read_image(img_name)\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1c2aa",
   "metadata": {},
   "source": [
    "## Benchmark the Jpeg Dataset\n",
    "In a real-life deep learning scenario, we have a dataloader which on every iteration returns images and labels of size `batch_size`. By running this for 1 epoch, we ensure that every images is seen at least once by the network. in this benchmarking example, only the timings of getting the images and pushing it to device is relevant. For now, there is no neural network step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ffec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(dataset, epochs, batch_size, num_workers, persistent_workers, pin_memory, device, shuffle=True, warm_start=False):\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            persistent_workers=num_workers > 0 and persistent_workers,\n",
    "            pin_memory=pin_memory,\n",
    "        )\n",
    "    \n",
    "    if not warm_start:\n",
    "            start = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if epoch == 1 and warm_start:\n",
    "            start = time.time()\n",
    "        timer_per_epoch = time.time()\n",
    "        for i, (images, labels) in enumerate(dataloader):\n",
    "            images, labels = map(\n",
    "                    lambda tensor: tensor.to(device, non_blocking=pin_memory),\n",
    "                    (images, labels),\n",
    "                )\n",
    "\n",
    "        print(f\"Epoch {epoch} finished in {time.time() - timer_per_epoch}\")\n",
    "    total_time = time.time() - start\n",
    "    return total_time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b95dc3",
   "metadata": {},
   "source": [
    "## Initialize some dataloading parameters\n",
    "Read careful and feel free to experiment with this\n",
    "\n",
    "We include a transformation to imitate real-life applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2ae3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(new_size, to_tensor=False):\n",
    "    transform_list = []\n",
    "    if to_tensor:\n",
    "        transform_list.append(transforms.ToTensor())\n",
    "    transform_list.append(transforms.Resize((new_size, new_size), antialias=True))\n",
    "    transform_list.append(transforms.RandomHorizontalFlip())\n",
    "    transform = transforms.Compose(transform_list)\n",
    "    return transform\n",
    "\n",
    "\n",
    "dataloader_kwargs = {\n",
    "    \"epochs\": 2,\n",
    "    \"num_workers\": 1,\n",
    "    \"batch_size\": 16,\n",
    "    \"device\": \"cpu\",\n",
    "    \"persistent_workers\": True,\n",
    "    \"warm_start\": False,\n",
    "    \"pin_memory\": True,\n",
    "    \"shuffle\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bc5aa1",
   "metadata": {},
   "source": [
    "## Run the benchmark on the Jpeg data\n",
    "Below, we run the benchmark for 2 epochs and return the timings for each epoch to see the speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cdb23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = f\"{DATA_PATH}/disk_transformed\"\n",
    "\n",
    "transform_jpeg = transform(new_size=256, to_tensor=True)\n",
    "jpeg_dataset = JpegDataset(img_path, transform=transform_jpeg)\n",
    "\n",
    "jpeg_time = benchmark(jpeg_dataset, **dataloader_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2859e0",
   "metadata": {},
   "source": [
    "## Cool, what now\n",
    "As we have already discussed, storing and loading a lot of individual files can lead to poor performance for both the filesystem as well as your network training. On top, copying the data from one space to another is also faster if done with packed data formats. So let's convert them!\n",
    "\n",
    "Wait, when you download a dataset and it already comes into a zip or tar folder, you already have your packed formats. While other alternatives exist (hdf5, lmdb, petastorm), using the original zip or tar folder might already be enough. Or if you need to split the data, you can create a new zip folder without ever needing to completely extract all individual files onto your machine.\n",
    "\n",
    "Instead of a Jpeg dataset class, let's make a dataclass for zip files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aa239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZIPDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"ImageNet10k Dataset for packed ZIP to pass to a PyTorch dataloader\n",
    "\n",
    "    Args:\n",
    "        path (str): location where the images are stored on disk\n",
    "        transform (obj): torchvision.transforms object or None\n",
    "        load_encoded (bool): whether the images within the .zip file are encoded or saved as bytes directly\n",
    "    Returns:\n",
    "        torch Dataset: to pass to a dataloader\n",
    "    \"\"\"\n",
    "    def __init__(self, path, transform=None, load_encoded=False):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.load_encoded = load_encoded\n",
    "\n",
    "        self.zip_file = zipfile.ZipFile(path)\n",
    "        self.members = sorted(self.zip_file.namelist())\n",
    "\n",
    "        self._get_img_names()\n",
    "\n",
    "        label_fname = \"dataset.json\"\n",
    "        self._get_labels(label_fname)\n",
    "\n",
    "    def _get_img_names(self):\n",
    "        PIL.Image.init()\n",
    "        self.img_names = [m for m in self.members if m.lower().endswith(tuple(PIL.Image.EXTENSION.keys()))]\n",
    "\n",
    "    def _get_labels(self, label_fname):\n",
    "        label_file = self.zip_file.open(label_fname, \"r\")\n",
    "        self.labels = json.load(label_file)[\"labels\"]\n",
    "\n",
    "\n",
    "    def _get_image(self, img_fname, shape=(3, 256, 256)):\n",
    "        img_f = self.zip_file.open(img_fname, \"r\")\n",
    "        if self.load_encoded:\n",
    "            image = PIL.Image.open(img_f).convert(\"RGB\")\n",
    "        else:\n",
    "            buffer = np.frombuffer(img_f.read(), dtype=np.uint8).reshape(shape)\n",
    "            image = torch.tensor(buffer) # make writeable copy\n",
    "        return image\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_fname = self.img_names[index]\n",
    "        image = self._get_image(img_fname)\n",
    "        label = self.labels[img_fname]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09eeb08",
   "metadata": {},
   "source": [
    "## Benchmark the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f26cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = f\"{DATA_PATH}/data/zip/part0.zip\"\n",
    "transform_zip = transform(new_size=256, to_tensor=False)\n",
    "zip_dataset = ZIPDataset(zip_path, transform=transform_zip, load_encoded=False)\n",
    "zip_time = benchmark(zip_dataset, **dataloader_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3da2542",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_up = jpeg_time / zip_time\n",
    "print(f\"Speed up of zip dataset compared to jpeg dataset: {round(speed_up, 3)}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d13a689",
   "metadata": {},
   "source": [
    "We already see a notable speed-up in favor for the ZIP dataset. But we only run the dataloading with a single process. In practice, while training neural networks we want the model to fully saturate the accelerator (GPU). We do this by providing the data as fast as possible such that the GPU is not waiting for the data and is bottlenecked by IO.\n",
    "\n",
    "Let's change the dataloader settings to increase the number of processes to say 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d718c6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the number of workers to 8\n",
    "dataloader_kwargs[\"num_workers\"] = 8\n",
    "# Re-run the JPEG dataset benchmark and \n",
    "jpeg_time_multi = benchmark(jpeg_dataset, **dataloader_kwargs)\n",
    "# Re-run the ZIP dataset benchmark and call the benchmark time\n",
    "zip_time_multi = benchmark(zip_dataset, **dataloader_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eec4203",
   "metadata": {},
   "source": [
    "If everything went well, it didn't go well. We got a weird BadZipFile error from ZIP with two file names having different headers or a bad magic number for file header... What could this be?\n",
    "\n",
    "\n",
    "What is happening is that the images are all zipped in a single ZIP file. While loading the data, we set the number of processes that read the data to 8 (where did we set it?). All these processes are trying to open and read from the same ZIP file. Since we have opened a single connection to the ZIP file in the initialization of the dataset, Python only allows us to access this file through this file handle but since 8 workers are trying at the same time we have interference between the processes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffade99",
   "metadata": {},
   "source": [
    "# Multiple worker support\n",
    "\n",
    "Now we will increase the amount of workers to access the same zip-file.\n",
    "\n",
    "\n",
    "Upon opening a file within the zip, we will get a file handle from which we will read our images/labels.\n",
    "You can see a zip-file handle as a separate file, as it is a place within the zip file where this image/label is located.\n",
    "This also means that we cannot store a single file-handle when accessing multiple files from within this zip.\n",
    "Instead, we need a separate file-handle for each file we are accessing.\n",
    "As we will want multiple workers to do different work in parallel, we want them to each access their own files, and as such speed up this process.\n",
    "Every worker will end up with their own file handle (which are not the same) to each process a separate file.\n",
    "\n",
    "The implementation for this requires the tracking of multiple zip-file handles, or handles, such that the workers do not interfere with each others files, and also do not do the same work multiple times.\n",
    "We will do this by changing the torch dataset by passing every worker their own handle.\n",
    "The communication to the zip file will go through this handle, which means that this must be a separate one for each worker.\n",
    "\n",
    "This is done in the following four steps:\n",
    " - Create the workers\n",
    " - Get the information about the workers (the worker-id)\n",
    " - Give each worker a separate handle\n",
    " - Read the images and labels from this handle\n",
    " \n",
    " Tips:\n",
    " - Each workers independently initializes their own MultiWorkerZIPDataset instance.  \n",
    " - Fill in the new `_get_file_handle` function\n",
    " - Revise the code from the previous dataset for loading the labels and images\n",
    " \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac875d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiWorkerZIPDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"ImageNet10k Dataset for packed ZIP to pass to a PyTorch dataloader\n",
    "\n",
    "    Args:\n",
    "        path (str): location where the images are stored on disk\n",
    "        transform (obj): torchvision.transforms object or None\n",
    "        load_encoded (bool): whether the images within the .zip file are encoded or saved as bytes directly\n",
    "    Returns:\n",
    "        torch Dataset: to pass to a dataloader\n",
    "    \"\"\"\n",
    "    def __init__(self, path, transform=None, load_encoded=False):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.load_encoded = load_encoded\n",
    "\n",
    "        # Each worker needs access to get the keys/filenames of the ZIP file\n",
    "        worker = torch.utils.data.get_worker_info()\n",
    "        worker = worker.id if worker else None\n",
    "        self.zip_handle = {worker: zipfile.ZipFile(path)}\n",
    "        self.members = sorted(self.zip_handle[worker].namelist())\n",
    "\n",
    "        self._get_img_names()\n",
    "\n",
    "        label_fname = \"dataset.json\"\n",
    "        self._get_labels(worker, label_fname)\n",
    "\n",
    "    def _get_img_names(self):\n",
    "        PIL.Image.init()\n",
    "        self.img_names = [m for m in self.members if m.lower().endswith(tuple(PIL.Image.EXTENSION.keys()))]\n",
    "\n",
    "    def _get_labels(self, worker, label_fname):\n",
    "        label_file = self.zip_handle[worker].open(label_fname, \"r\")\n",
    "        self.labels = json.load(label_file)[\"labels\"]\n",
    "\n",
    "    def _get_file_handle(self, fname):\n",
    "        # TODO: \n",
    "        # This function must return the file handle for the current worker, \n",
    "        # or create one if it is not already created\n",
    "        worker = torch.utils.data.get_worker_info()\n",
    "        worker = worker.id if worker else None\n",
    "\n",
    "        if worker not in self.zip_handle:\n",
    "            self.zip_handle[worker] = zipfile.ZipFile(self.path)\n",
    "\n",
    "        return self.zip_handle[worker].open(fname, \"r\")\n",
    "\n",
    "    def _get_image(self, img_fname, shape=(3, 256, 256)):\n",
    "        img_f = self._get_file_handle(img_fname)\n",
    "        if self.load_encoded:\n",
    "            image = PIL.Image.open(img_f).convert(\"RGB\")\n",
    "        else:\n",
    "            buffer = np.frombuffer(img_f.read(), dtype=np.uint8).reshape(shape)\n",
    "            image = torch.tensor(buffer) # make writeable copy\n",
    "        return image\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_fname = self.img_names[index]\n",
    "        image = self._get_image(img_fname)\n",
    "        label = self.labels[img_fname]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Clean all file handles of the workers on exit\"\"\"\n",
    "        if hasattr(self, \"zip_handle\"):\n",
    "            for o in self.zip_handle.values():\n",
    "                o.close()\n",
    "\n",
    "    def __getstate__(self):\n",
    "        \"\"\"Serialize without the ZipFile references, for multiprocessing compatibility\"\"\"\n",
    "        state = dict(self.__dict__)\n",
    "        state[\"zip_handle\"] = {}\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c7deb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_dataset_multi = MultiWorkerZIPDataset(zip_path, transform=transform_zip, load_encoded=False)\n",
    "zip_time_multi = benchmark(zip_dataset_multi, **dataloader_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d5742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_up = jpeg_time_multi / zip_time_multi\n",
    "print(f\"Speed up of zip dataset compared to jpeg dataset: {round(speed_up, 3)}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360df97",
   "metadata": {},
   "source": [
    "# Evaluate... \n",
    "We can see that zipping our datasets works in favor in terms of dataloading. This is a crucial step in training your machine learning pipeline and can be a bottleneck. With the help of profiling and using packed data format we can minimize the bottleneck for IO communication between files and the neural network. On top, packing your data encourages a healthy filesystem for you and others :)\n",
    "\n",
    "\n",
    "### Last things\n",
    "Try to change the number of workers to any number between 1 and 32, see what happens with the speed-up!\n",
    "\n",
    "Other tips:\n",
    "- Any transformation on your dataset which is deterministic (for example resize) can be done once in advance as preprocessing and will probably save you time during the actual training.\n",
    "- Look into LMDB/HDF5/Petastorm to see if there is a packed data format which may be more suited for your data\n",
    "- More info here: https://servicedesk.surf.nl/wiki/display/WIKI/Best+Practice+for+Data+Formats+in+Deep+Learning and here: \\newline https://github.com/sara-nl/Packed-Data-Formats\n",
    "\n",
    "\n",
    "Finally, it's good to keep in mind that reading files is a task which we can partially do in a overlapping way with GPU computations. In this hands-out, we did not consider a neural network so the GPU was idle at all times. In real-life scenarios, we can push the data to the GPU, let the GPU do its calculations and at the same time load some more data for the next batch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32431f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
